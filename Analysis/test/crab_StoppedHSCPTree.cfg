#
# Use this CRAB config for generating stopped HSCP Trees
# Multiple options for filtering, re-running HLT etc
# Jim Brooke, Sept 09
# 


[CRAB]

jobtype = cmssw
scheduler = glitecoll
### NOTE: just setting the name of the server (pi, lnl etc etc ) 
###       crab will submit the jobs to the server...   
#server_name = cnaf

[CMSSW]

### The data you want to access (to be found on DBS)

# specially for running on private skim & re-reco
#datasetpath=None

datasetpath=/MinimumBias/BeamCommissioning09-18thFebPreProd_351p1_Skim_StoppedHSCP-v2/RAW-RECO
runselection=124022

#datasetpath=/MinimumBias/BeamCommissioning09-v1/RAW
#runselection=124022,124023,124024,124025,124026,124027,124029,124030

# CRAFT 09 HLT re-skim
#dbs_url = http://cmsdbsprod.cern.ch/cms_dbs_ph_analysis_02/servlet/DBSServlet
#datasetpath=/Calo/jbrooke-StoppedHSCP_CRAFT09_rerunHLT_v5-a9f20537e9e1239e6910ee9cb81f358d/USER

# CRAFT 09
#datasetpath=/Calo/CRAFT09-GR09_31X_V5P_StoppedHSCP-332_v4/RAW-RECO
#runselection=110958,110972,110987,110998,111009,111039,111138

# CRAFT 08
#datasetpath=/Calo/Commissioning08-v1/RAW
#runselection=68021

### The ParameterSet you want to use
pset=test/stoppedHSCPTree.py

### Splitting parameters
total_number_of_events=-1
events_per_job = 100000
#number_of_jobs = 1

### The output files (comma separated list)
output_file = stoppedHSCPTree.root
	
[USER]

### OUTPUT files Management
##  output back into UI 
return_data = 0

### OUTPUT files INTO A SE
copy_data = 1
#storage_element          = lcgse02.phy.bris.ac.uk
#storage_path             = /srm/managerv2?SFN=/cms
#storage_port             = 8444
#user_remote_dir          = /store/user/jbrooke/StoppedHSCP_CRAFT09_336_v5

storage_element          = T2_UK_SGrid_Bristol
user_remote_dir          = StoppedHSCPTree_351p1_preprod

ui_working_dir = StoppedHSCPTree_351p1_preprod

#if server 
thresholdLevel = 50
eMail = jim.brooke@cern.ch

[GRID]

## RB/WMS management:
rb = CERN
proxy_server = myproxy.cern.ch

##  Black and White Lists management:
## By Storage
#se_black_list = 
#se_white_list = 

## By ComputingElement 
#ce_black_list = 
#ce_white_list = heplnx206.pp.rl.ac.uk,heplnx207.pp.rl.ac.uk
#ce_white_list = lcgce03.phy.bris.ac.uk, lcgce02.phy.bris.ac.uk, lcgce01.phy.bris.ac.uk 

[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))

